import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
import xgboost as xgb

# Load your data (replace this with your dataset)
# Example: Load customer churn data from a CSV file
# data = pd.read_csv('customer_churn.csv')

# For demonstration purposes, let's create a sample dataset
np.random.seed(42)
data = pd.DataFrame({
    'Age': np.random.randint(18, 70, 1000),
    'Annual_Income': np.random.randint(30000, 120000, 1000),
    'Subscription_Type': np.random.choice(['Basic', 'Premium'], size=1000),
    'Tenure': np.random.randint(1, 12, 1000),
    'Churn': np.random.choice([0, 1], size=1000)  # 1 = Churn, 0 = No Churn
})

# Display the first few rows
print(data.head())

# Data Preprocessing: Convert categorical variables to numerical (e.g., Subscription_Type)
data = pd.get_dummies(data, drop_first=True)

# Split data into features (X) and target (y)
X = data.drop('Churn', axis=1)
y = data['Churn']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (important for models like Logistic Regression)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model 1: Logistic Regression
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)

# Model 2: Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred_rf = rf_classifier.predict(X_test)

# Model 3: XGBoost Classifier
xgb_classifier = xgb.XGBClassifier(random_state=42)
xgb_classifier.fit(X_train, y_train)
y_pred_xgb = xgb_classifier.predict(X_test)

# Evaluate the models: Print classification reports and ROC-AUC scores
def evaluate_model(y_true, y_pred, model_name):
    print(f"\n{model_name} Classification Report:")
    print(classification_report(y_true, y_pred))
    print(f"ROC-AUC Score: {roc_auc_score(y_true, y_pred)}")

evaluate_model(y_test, y_pred_log_reg, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest")
evaluate_model(y_test, y_pred_xgb, "XGBoost")

# Confusion Matrices
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No Churn", "Churn"], yticklabels=["No Churn", "Churn"])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

plot_confusion_matrix(y_test, y_pred_log_reg, "Logistic Regression")
plot_confusion_matrix(y_test, y_pred_rf, "Random Forest")
plot_confusion_matrix(y_test, y_pred_xgb, "XGBoost")

# Cross-validation (optional but helps to assess model's stability)
def cross_validation(model, X, y, model_name):
    cv_scores = cross_val_score(model, X, y, cv=5)
    print(f"{model_name} Cross-Validation Scores: {cv_scores}")
    print(f"Mean CV Score: {cv_scores.mean()}")
    print(f"Standard Deviation of CV Score: {cv_scores.std()}")

cross_validation(log_reg, X_train, y_train, "Logistic Regression")
cross_validation(rf_classifier, X_train, y_train, "Random Forest")
cross_validation(xgb_classifier, X_train, y_train, "XGBoost")

# Feature Importance (Random Forest and XGBoost)
def plot_feature_importance(model, X_columns, model_name):
    importance = model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'Feature': X_columns,
        'Importance': importance
    }).sort_values(by='Importance', ascending=False)
    
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
    plt.title(f'Feature Importance - {model_name}')
    plt.show()

plot_feature_importance(rf_classifier, X.columns, "Random Forest")
plot_feature_importance(xgb_classifier, X.columns, "XGBoost")
